{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style='ticks', context='paper')\n",
    "import numpy as np\n",
    "from textwrap import wrap\n",
    "from util import color_list,linestyle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"/data2/lichenni/ns3\"\n",
    "file2matrix = {\n",
    "        '../../workload/spatials/cluster_a.json': 'Traffic Matrix A',\n",
    "        '../../workload/spatials/cluster_b.json': 'Traffic Matrix B',\n",
    "        '../../workload/spatials/cluster_c.json': 'Traffic Matrix C',\n",
    "    }\n",
    "file2oversub = {\n",
    "        'spec/cluster_1_to_1.json': '1-to-1',\n",
    "        'spec/cluster_2_to_1.json': '2-to-1',\n",
    "        'spec/cluster_4_to_1.json': '4-to-1',\n",
    "    }\n",
    "\n",
    "P99_PERCENTILE_LIST = np.arange(1, 101, 1)\n",
    "\n",
    "MTU=1000\n",
    "BDP = 10 * MTU\n",
    "bin_size_list=[MTU, BDP, 5 * BDP]\n",
    "labels = {0: '0<size<=MTU', 1:'MTU<size<=BDP', 2:'BDP<size<=5BDP', 3:'5BDP<size'}\n",
    "\n",
    "n_size_bucket_list_output=len(bin_size_list)+1\n",
    "n_percentiles=len(P99_PERCENTILE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% load\n",
    "N_FLOWS=500*100*4\n",
    "N_FLOWS_PER_PATH=400\n",
    "N_FLOW_THRESHOLD = [1, 1, 1, 1]\n",
    "worst_low_id=2\n",
    "mlsys_dir=\"mlsys\"\n",
    "mix_dir = f'../data/{worst_low_id}'\n",
    "# Accuracy metrics\n",
    "df_ns3 = pd.read_csv(f'{mix_dir}/ns3/records.csv')\n",
    "df_pmn_m = pd.read_csv(f'{mix_dir}/pmn-m/records.csv')\n",
    "df_mlsys = [[] for _ in range(n_size_bucket_list_output)]\n",
    "\n",
    "n_freq_list=[]\n",
    "n_flow_list=[]\n",
    "sizes=df_pmn_m['size']\n",
    "\n",
    "path_idx=0\n",
    "while os.path.exists(f'{mix_dir}/{mlsys_dir}/{path_idx}/fct_mlsys.txt'):\n",
    "    with open(f'{mix_dir}/{mlsys_dir}/path_{path_idx}.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data=lines[0].strip().split(\",\")\n",
    "        n_freq_list.append(int(data[-1]))\n",
    "        \n",
    "        flowid_list=[int(tmp) for tmp in lines[2].strip().split(\",\")]\n",
    "        size_list=[sizes[flowid] for flowid in flowid_list]\n",
    "        \n",
    "        n_links=len(data[0].split(\"|\"))-1\n",
    "        tmp=np.digitize(size_list, bin_size_list)\n",
    "        # Count occurrences of each bin index\n",
    "        bin_counts = np.zeros(n_size_bucket_list_output)\n",
    "        for bin_idx in tmp:\n",
    "            bin_counts[bin_idx]+=1\n",
    "        n_flow_list.append(bin_counts)\n",
    "    path_idx+=1\n",
    "n_flow_list=np.array(n_flow_list)\n",
    "n_flow_list_sum=n_flow_list.sum(axis=0)\n",
    "\n",
    "with open(f'{mix_dir}/{mlsys_dir}/path.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines=lines[1:]\n",
    "    for line_idx,line in enumerate(lines):\n",
    "        data=line.strip().split(\",\")\n",
    "        data = [float(value) for value in data]\n",
    "        n_freq=n_freq_list[line_idx//n_size_bucket_list_output]\n",
    "        \n",
    "        if n_flow_list[line_idx//n_size_bucket_list_output][line_idx%n_size_bucket_list_output]>=N_FLOW_THRESHOLD[line_idx%n_size_bucket_list_output]:\n",
    "            for _ in range(n_freq):\n",
    "                df_mlsys[line_idx%n_size_bucket_list_output].extend(data)\n",
    "df_mlsys_shape=[len(df_mlsys[i]) for i in range(len(df_mlsys))]\n",
    "\n",
    "sizes=df_ns3['size']\n",
    "bin=np.digitize(sizes, bin_size_list)\n",
    "\n",
    "bin_counts = np.bincount(bin)\n",
    "total_count = np.sum(bin_counts)\n",
    "# # Calculate the ratio for each bucket\n",
    "bucket_ratios = bin_counts / total_count\n",
    "print(\"bucket_ratios: \",bucket_ratios)\n",
    "bucket_ratios_sampled=n_flow_list_sum/sum(n_flow_list_sum)\n",
    "print(\"bucket_ratios_sampled: \",bucket_ratios_sampled)\n",
    "            \n",
    "df_mlsys_bucket=np.array([np.percentile(df_mlsys[i],99) for i in range(len(df_mlsys))])\n",
    "print(\"df_mlsys_bucket: \",df_mlsys_bucket)\n",
    "df_mlsys_total=[]\n",
    "for i in range(len(df_mlsys)):\n",
    "    df_mlsys_total.extend(df_mlsys[i])\n",
    "df_mlsys_p99=np.percentile(df_mlsys_total,99)\n",
    "\n",
    "sldn_ns3=df_ns3['slowdown']\n",
    "sldn_pmn_m=df_pmn_m['slowdown']\n",
    "sldn_ns3_p99=np.percentile(sldn_ns3,99)\n",
    "sldn_pmn_m_p99=np.percentile(sldn_pmn_m,99)\n",
    "\n",
    "print(\"sldn_ns3: \",sldn_ns3_p99,\" sldn_pmn_m: \", sldn_pmn_m_p99,\" df_mlsys: \", df_mlsys_p99)\n",
    "error=(sldn_pmn_m_p99-sldn_ns3_p99)/sldn_ns3_p99\n",
    "error_mlsys=(df_mlsys_p99-sldn_ns3_p99)/sldn_ns3_p99\n",
    "\n",
    "print(f\"df_ns3: {df_ns3.shape[0]}, df_pmn_m: {df_pmn_m.shape[0]}, df_mlsys: {df_mlsys_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fontsize=15\n",
    "plt.figure(0,figsize=(12, 6))\n",
    "bin_ns3=np.digitize(df_ns3['size'], bin_size_list)\n",
    "bin_pmn_m=np.digitize(df_pmn_m['size'], bin_size_list)\n",
    "plt.suptitle(f\"Slowdown CDF for mix-{worst_low_id} in {mlsys_dir}\\npmn_error={error}, mlsys_error={error_mlsys}\",fontsize=_fontsize+5)\n",
    "plt.rcParams['legend.fontsize'] = _fontsize\n",
    "for i in range(len(labels)):\n",
    "    tmp_sldn_ns3 = np.extract(bin_ns3==i, sldn_ns3)\n",
    "    tmp_sldn_pmn_m = np.extract(bin_pmn_m==i, sldn_pmn_m)\n",
    "    tmp_sldn_mlsys=df_mlsys[i]\n",
    "    print(f\"{i}: \", min(tmp_sldn_mlsys))\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f\"{labels[i]}-#{len(tmp_sldn_ns3)}\",fontsize=_fontsize)\n",
    "    plt.plot(np.sort(tmp_sldn_ns3), (np.arange(len(tmp_sldn_ns3))/len(tmp_sldn_ns3)*100), label='ns3', color='blue', linewidth=3)\n",
    "    plt.plot(np.sort(tmp_sldn_pmn_m), (np.arange(len(tmp_sldn_pmn_m))/len(tmp_sldn_pmn_m)*100), label='pmn-m', color='red', linewidth=3)\n",
    "    plt.plot(np.sort(tmp_sldn_mlsys), (np.arange(len(tmp_sldn_mlsys))/len(tmp_sldn_mlsys)*100), label='mlsys', color='green', linewidth=3)\n",
    "    # plt.plot(np.sort(tmp_sldn_flowsim), (np.arange(len(tmp_sldn_flowsim))/len(tmp_sldn_flowsim)*100), label='flowsim', color='orange', linewidth=3)\n",
    "    plt.axhline(99, color='green', linewidth=0.5)\n",
    "    # plt.xscale('log')\n",
    "    plt.ylim(80, 100)\n",
    "    plt.xlabel('slow-down', fontsize=_fontsize)\n",
    "    plt.yticks(fontsize=_fontsize)\n",
    "    plt.xticks(fontsize=_fontsize)\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% load\n",
    "N_FLOWS=500*100*4\n",
    "N_FLOWS_PER_PATH=400\n",
    "N_FLOW_THRESHOLD = [1, 1, 1, 1]\n",
    "worst_low_id=1\n",
    "mlsys_dir=\"mlsys-new_e426\"\n",
    "mix_dir = f'../data/{worst_low_id}'\n",
    "# Accuracy metrics\n",
    "df_ns3 = pd.read_csv(f'{mix_dir}/ns3/records.csv')\n",
    "df_pmn_m = pd.read_csv(f'{mix_dir}/pmn-m/records.csv')\n",
    "df_mlsys = [[] for _ in range(n_size_bucket_list_output)]\n",
    "\n",
    "n_freq_list=[]\n",
    "n_flow_list=[]\n",
    "sizes=df_pmn_m['size']\n",
    "\n",
    "path_idx=0\n",
    "while os.path.exists(f'{mix_dir}/{mlsys_dir}/{path_idx}/fct_mlsys.txt'):\n",
    "    with open(f'{mix_dir}/{mlsys_dir}/path_{path_idx}.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data=lines[0].strip().split(\",\")\n",
    "        n_freq_list.append(int(data[-1]))\n",
    "        \n",
    "        flowid_list=[int(tmp) for tmp in lines[2].strip().split(\",\")]\n",
    "        size_list=[sizes[flowid] for flowid in flowid_list]\n",
    "        \n",
    "        n_links=len(data[0].split(\"|\"))-1\n",
    "        tmp=np.digitize(size_list, bin_size_list)\n",
    "        # Count occurrences of each bin index\n",
    "        bin_counts = np.zeros(n_size_bucket_list_output)\n",
    "        for bin_idx in tmp:\n",
    "            bin_counts[bin_idx]+=1\n",
    "        n_flow_list.append(bin_counts)\n",
    "    path_idx+=1\n",
    "n_flow_list=np.array(n_flow_list)\n",
    "n_flow_list_sum=n_flow_list.sum(axis=0)\n",
    "\n",
    "with open(f'{mix_dir}/{mlsys_dir}/path.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines=lines[1:]\n",
    "    for line_idx,line in enumerate(lines):\n",
    "        data=line.strip().split(\",\")\n",
    "        data = [float(value) for value in data]\n",
    "        n_freq=n_freq_list[line_idx//n_size_bucket_list_output]\n",
    "        \n",
    "        if n_flow_list[line_idx//n_size_bucket_list_output][line_idx%n_size_bucket_list_output]>=N_FLOW_THRESHOLD[line_idx%n_size_bucket_list_output]:\n",
    "            for _ in range(n_freq):\n",
    "                df_mlsys[line_idx%n_size_bucket_list_output].extend(data)\n",
    "df_mlsys_shape=[len(df_mlsys[i]) for i in range(len(df_mlsys))]\n",
    "\n",
    "sizes=df_ns3['size']\n",
    "bin=np.digitize(sizes, bin_size_list)\n",
    "\n",
    "bin_counts = np.bincount(bin)\n",
    "total_count = np.sum(bin_counts)\n",
    "# # Calculate the ratio for each bucket\n",
    "bucket_ratios = bin_counts / total_count\n",
    "print(\"bucket_ratios: \",bucket_ratios)\n",
    "bucket_ratios_sampled=n_flow_list_sum/sum(n_flow_list_sum)\n",
    "print(\"bucket_ratios_sampled: \",bucket_ratios_sampled)\n",
    "            \n",
    "df_mlsys_bucket=np.array([np.percentile(df_mlsys[i],99) for i in range(len(df_mlsys))])\n",
    "print(\"df_mlsys_bucket: \",df_mlsys_bucket)\n",
    "# df_mlsys_p99=np.sum(np.multiply(df_mlsys_bucket.T, bucket_ratios_sampled).T,axis=0)\n",
    "df_mlsys_total=[]\n",
    "for i in range(len(df_mlsys)):\n",
    "    df_mlsys_total.extend(df_mlsys[i])\n",
    "    # n_tmp=int(N_FLOWS*bucket_ratios_sampled[i])\n",
    "    # df_mlsys_total.extend(np.random.choice(df_mlsys[i],n_tmp,replace=True))\n",
    "df_mlsys_p99=np.percentile(df_mlsys_total,99)\n",
    "\n",
    "sldn_ns3=df_ns3['slowdown']\n",
    "sldn_pmn_m=df_pmn_m['slowdown']\n",
    "sldn_ns3_p99=np.percentile(sldn_ns3,99)\n",
    "sldn_pmn_m_p99=np.percentile(sldn_pmn_m,99)\n",
    "\n",
    "print(\"sldn_ns3: \",sldn_ns3_p99,\" sldn_pmn_m: \", sldn_pmn_m_p99,\" df_mlsys: \", df_mlsys_p99)\n",
    "error=(sldn_pmn_m_p99-sldn_ns3_p99)/sldn_ns3_p99\n",
    "error_mlsys=(df_mlsys_p99-sldn_ns3_p99)/sldn_ns3_p99\n",
    "\n",
    "# assert df_ns3.shape[0]==df_pmn_m.shape[0]==df_ns3_path.shape[0]==sldn_flowsim.shape[0]\n",
    "print(f\"df_ns3: {df_ns3.shape[0]}, df_pmn_m: {df_pmn_m.shape[0]}, df_mlsys: {df_mlsys_shape}\")\n",
    "# assert df_ns3.shape[0]==df_pmn_m.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fontsize=15\n",
    "plt.figure(0,figsize=(12, 6))\n",
    "plt.suptitle(f\"Slowdown CDF for mix-{worst_low_id} in {mlsys_dir}\\npmn_error={error}, mlsys_error={error_mlsys}\",fontsize=_fontsize+5)\n",
    "plt.rcParams['legend.fontsize'] = _fontsize\n",
    "bin_ns3=np.digitize(df_ns3['size'], bin_size_list)\n",
    "bin_pmn_m=np.digitize(df_pmn_m['size'], bin_size_list)\n",
    "for i in range(len(labels)):\n",
    "    tmp_sldn_ns3 = np.extract(bin_ns3==i, sldn_ns3)\n",
    "    tmp_sldn_pmn_m = np.extract(bin_pmn_m==i, sldn_pmn_m)\n",
    "    tmp_sldn_mlsys=df_mlsys[i]\n",
    "    print(f\"{i}: \", min(tmp_sldn_mlsys))\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f\"{labels[i]}-#{len(tmp_sldn_ns3)}\",fontsize=_fontsize)\n",
    "    plt.plot(np.sort(tmp_sldn_ns3), (np.arange(len(tmp_sldn_ns3))/len(tmp_sldn_ns3)*100), label='ns3', color='blue', linewidth=3)\n",
    "    plt.plot(np.sort(tmp_sldn_pmn_m), (np.arange(len(tmp_sldn_pmn_m))/len(tmp_sldn_pmn_m)*100), label='pmn-m', color='red', linewidth=3)\n",
    "    plt.plot(np.sort(tmp_sldn_mlsys), (np.arange(len(tmp_sldn_mlsys))/len(tmp_sldn_mlsys)*100), label='mlsys', color='green', linewidth=3)\n",
    "    # plt.axhline(99, color='green', linewidth=0.5)\n",
    "    # plt.xscale('log')\n",
    "    plt.ylim(90, 100)\n",
    "    plt.xlim(0,10)\n",
    "    plt.xlabel('slow-down', fontsize=_fontsize)\n",
    "    plt.yticks(fontsize=_fontsize)\n",
    "    plt.xticks(fontsize=_fontsize)\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf(\n",
    "    raw_data,\n",
    "    file_name,\n",
    "    linelabels,\n",
    "    x_label,\n",
    "    y_label=\"CDF\",\n",
    "    log_switch=False,\n",
    "    rotate_xaxis=False,\n",
    "    ylim_low=0,\n",
    "    xlim=None,\n",
    "    xlim_bottom=None,\n",
    "    fontsize=15,\n",
    "    legend_font=15,\n",
    "    loc=2,\n",
    "    title=None,\n",
    "    enable_abs=False,\n",
    "    group_size=1,\n",
    "):\n",
    "    _fontsize = fontsize\n",
    "    fig = plt.figure(figsize=(6, 2.))  # 2.5 inch for 1/3 double column width\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    plt.axhline(0.99, color='k', linewidth=3, linestyle='--',zorder=0)\n",
    "    \n",
    "    ax.tick_params(axis=\"y\", direction=\"in\")\n",
    "    ax.tick_params(axis=\"x\", direction=\"in\")\n",
    "    if log_switch:\n",
    "        ax.set_xscale(\"log\")\n",
    "\n",
    "    plt.ylabel(y_label, fontsize=_fontsize)\n",
    "    plt.xlabel(x_label, fontsize=_fontsize)\n",
    "    linelabels = [\"\\n\".join(wrap(l, 30)) for l in linelabels]\n",
    "    for i in range(len(raw_data)):\n",
    "        data = raw_data[i]\n",
    "        data = data[~np.isnan(data)]\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        if enable_abs:\n",
    "            data = abs(data)\n",
    "        # data=random.sample(data,min(1e6,len(data)))\n",
    "        data_size = len(data)\n",
    "        # data=list(filter(lambda score: 0<=score < std_val, data))\n",
    "        # Set bins edges\n",
    "        data_set = sorted(set(data))\n",
    "        bins = np.append(data_set, data_set[-1] + 1)\n",
    "\n",
    "        # Use the histogram function to bin the data\n",
    "        counts, bin_edges = np.histogram(data, bins=bins, density=False)\n",
    "\n",
    "        counts = counts.astype(float) / data_size\n",
    "\n",
    "        # Find the cdf\n",
    "        cdf = np.cumsum(counts)\n",
    "\n",
    "        # Plot the cdf\n",
    "        if i < len(linelabels):\n",
    "            plt.plot(\n",
    "                bin_edges[0:-1],\n",
    "                cdf,\n",
    "                linestyle=linestyle_list[(i // group_size) % len(linestyle_list)],\n",
    "                color=color_list[(i % group_size) % len(color_list)],\n",
    "                label=linelabels[i],\n",
    "                linewidth=3,\n",
    "            )\n",
    "        else:\n",
    "            plt.plot(\n",
    "                bin_edges[0:-1],\n",
    "                cdf,\n",
    "                linestyle=linestyle_list[(i // group_size) % len(linestyle_list)],\n",
    "                color=color_list[(i % group_size) % len(color_list)],\n",
    "                linewidth=3,\n",
    "            )\n",
    "\n",
    "    legend_properties = {\"size\": legend_font}\n",
    "    plt.legend(\n",
    "        prop=legend_properties,\n",
    "        frameon=False,\n",
    "        loc=loc,\n",
    "    )\n",
    "\n",
    "    plt.ylim((ylim_low, 1))\n",
    "    if xlim_bottom:\n",
    "        plt.xlim(left=xlim_bottom)\n",
    "    if xlim:\n",
    "        plt.xlim(right=xlim)\n",
    "    # plt.tight_layout()\n",
    "    # plt.tight_layout(pad=0.5, w_pad=0.04, h_pad=0.01)\n",
    "    plt.yticks(fontsize=_fontsize)\n",
    "    plt.xticks(fontsize=_fontsize)\n",
    "    # plt.grid(True)\n",
    "    \n",
    "    if rotate_xaxis:\n",
    "        plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment=\"right\")\n",
    "    if title:\n",
    "        plt.title(title, fontsize=_fontsize - 5)\n",
    "    if file_name:\n",
    "        plt.savefig(file_name, bbox_inches=\"tight\", pad_inches=0)\n",
    "\n",
    "legend_list=['m3','Parsimon','ns-3']\n",
    "bin_ns3=np.digitize(df_ns3['size'], bin_size_list)\n",
    "bin_pmn_m=np.digitize(df_pmn_m['size'], bin_size_list)\n",
    "for i in range(len(labels)):\n",
    "    \n",
    "    tmp_sldn_ns3 = np.extract(bin_ns3==i, sldn_ns3)\n",
    "    tmp_sldn_pmn_m = np.extract(bin_pmn_m==i, sldn_pmn_m)\n",
    "    tmp_sldn_mlsys=np.array(df_mlsys[i])\n",
    "    \n",
    "    plot_data=[tmp_sldn_mlsys,tmp_sldn_pmn_m,tmp_sldn_ns3]\n",
    "    plot_cdf(plot_data,f'figs/eva_large_scale_{i}.pdf',legend_list,'FCT slowdown',legend_font=18,group_size=3,loc=4,rotate_xaxis=False,ylim_low=0.8,log_switch=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% load\n",
    "worst_low_id=1\n",
    "mlsys_dir=\"mlsys\"\n",
    "mix_dir = f'../data/{worst_low_id}'\n",
    "# df_ns3 = pd.read_csv(f'{mix_dir}/ns3/records.csv')\n",
    "# df_pmn_m = pd.read_csv(f'{mix_dir}/pmn-m/records.csv')\n",
    "df_ns3 = pd.read_csv(f'{mix_dir}/ns3/records.csv').sample(n=2_000_000)\n",
    "df_pmn_m = pd.read_csv(f'{mix_dir}/pmn-m/records.csv').sample(n=2_000_000)\n",
    "\n",
    "\n",
    "bin_ns3=np.digitize(df_ns3['size'], [10*MTU,100*MTU,1000*MTU])\n",
    "bin_pmn_m=np.digitize(df_pmn_m['size'], [10*MTU,100*MTU,1000*MTU])\n",
    "\n",
    "sldn_ns3=df_ns3['slowdown']\n",
    "sldn_pmn_m=df_pmn_m['slowdown']\n",
    "sldn_ns3_p99=np.percentile(sldn_ns3,99)\n",
    "sldn_pmn_m_p99=np.percentile(sldn_pmn_m,99)\n",
    "\n",
    "print(\"sldn_ns3: \",sldn_ns3_p99,\" sldn_pmn_m: \")\n",
    "\n",
    "legend_list=['ns-3','Parsimon']\n",
    "for i in range(len(labels)):\n",
    "    tmp_sldn_ns3 = np.extract(bin_ns3==i, sldn_ns3)\n",
    "    tmp_sldn_pmn_m = np.extract(bin_pmn_m==i, sldn_pmn_m)\n",
    "    \n",
    "    plot_data=[tmp_sldn_ns3,tmp_sldn_pmn_m]\n",
    "    plot_cdf(plot_data,None,legend_list,'FCT slowdown',legend_font=18,group_size=3,loc=4,rotate_xaxis=False,ylim_low=0.8,log_switch=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
