{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wasserstein_distance\n",
    "# sns.set(style='ticks', context='paper', font='CMU Sans Serif')\n",
    "sns.set(style='ticks', context='paper')\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from util import plot_cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=\"/data2/lichenni/ns3\"\n",
    "file2matrix = {\n",
    "        '../../workload/spatials/cluster_a.json': 'Traffic Matrix A',\n",
    "        '../../workload/spatials/cluster_b.json': 'Traffic Matrix B',\n",
    "        '../../workload/spatials/cluster_c.json': 'Traffic Matrix C',\n",
    "    }\n",
    "file2oversub = {\n",
    "        'spec/cluster_1_to_1.json': '1-to-1',\n",
    "        'spec/cluster_2_to_1.json': '2-to-1',\n",
    "        'spec/cluster_4_to_1.json': '4-to-1',\n",
    "    }\n",
    "\n",
    "P99_PERCENTILE_LIST = np.arange(1, 101, 1)\n",
    "\n",
    "MTU=1000\n",
    "BDP = 10 * MTU\n",
    "bin_size_list=[MTU, BDP, 5 * BDP]\n",
    "labels = {0: '0<size<=MTU', 1:'MTU<size<=BDP', 2:'BDP<size<=5BDP', 3:'5BDP<size'}\n",
    "\n",
    "n_size_bucket_list_output=len(bin_size_list)+1\n",
    "n_percentiles=len(P99_PERCENTILE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m mix_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworst_low_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Accuracy metrics\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m df_ns3 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmix_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ns3/records.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m df_pmn_m \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmix_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/pmn-m/records.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m df_mlsys \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_size_bucket_list_output)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Given one mix, get the relationship between the flow size buckets and the slowdown estiamtion error\n",
    "N_FLOWS=500*100*4\n",
    "N_FLOWS_PER_PATH=400\n",
    "N_FLOW_THRESHOLD = [1, 1, 1, 1]\n",
    "worst_low_id=2\n",
    "# mlsys_dir=\"mlsys_0114_const_bt10\"\n",
    "mlsys_dir=\"mlsys_final_reprod_v0\"\n",
    "# mlsys_dir=\"mlsys_bdp_bt1\"\n",
    "mix_dir = f'../data/{worst_low_id}'\n",
    "# Accuracy metrics\n",
    "df_ns3 = pd.read_csv(f'{mix_dir}/ns3/records.csv')\n",
    "df_pmn_m = pd.read_csv(f'{mix_dir}/pmn-m/records.csv')\n",
    "df_mlsys = [[] for _ in range(n_size_bucket_list_output)]\n",
    "\n",
    "n_freq_list=[]\n",
    "n_flow_list=[]\n",
    "sizes=df_pmn_m['size']\n",
    "\n",
    "# mix_dir = f'../data_test/{worst_low_id}'\n",
    "path_idx=0\n",
    "while os.path.exists(f'{mix_dir}/{mlsys_dir}/{path_idx}/fct_mlsys.txt'):\n",
    "    with open(f'{mix_dir}/{mlsys_dir}/path_{path_idx}.txt', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        data=lines[0].strip().split(\",\")\n",
    "        n_freq_list.append(int(data[-1]))\n",
    "        \n",
    "        flowid_list=[int(tmp) for tmp in lines[2].strip().split(\",\")]\n",
    "        size_list=[sizes[flowid] for flowid in flowid_list]\n",
    "        \n",
    "        n_links=len(data[0].split(\"|\"))-1\n",
    "        tmp=np.digitize(size_list, bin_size_list)\n",
    "        # tmp=np.digitize(size_list, bin_size_dict[n_links])\n",
    "        # Count occurrences of each bin index\n",
    "        bin_counts = np.zeros(n_size_bucket_list_output)\n",
    "        for bin_idx in tmp:\n",
    "            bin_counts[bin_idx]+=1\n",
    "        # n_flow_list.append(bin_counts>=N_FLOW_THRESHOLD)\n",
    "        n_flow_list.append(bin_counts)\n",
    "    path_idx+=1\n",
    "n_flow_list=np.array(n_flow_list)\n",
    "# print(\"n_flow_list: \",n_flow_list.shape)\n",
    "n_flow_list_sum=n_flow_list.sum(axis=0)\n",
    "# print(\"n_flow_list_sum: \",n_flow_list_sum)\n",
    "\n",
    "with open(f'{mix_dir}/{mlsys_dir}/path.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines=lines[1:]\n",
    "    for line_idx,line in enumerate(lines):\n",
    "        data=line.strip().split(\",\")\n",
    "        data = [float(value) for value in data]\n",
    "        n_freq=n_freq_list[line_idx//n_size_bucket_list_output]\n",
    "        \n",
    "        # prop_tmp=n_flow_list[line_idx//n_size_bucket_list_output]/np.sum(n_flow_list[line_idx//n_size_bucket_list_output])\n",
    "        # num_tmp=int(N_FLOWS_PER_PATH*prop_tmp[line_idx%n_size_bucket_list_output])\n",
    "        # data_sampled=np.random.choice(data,num_tmp,replace=True)\n",
    "        # for _ in range(n_freq):\n",
    "        #     df_mlsys[line_idx%n_size_bucket_list_output].extend(data_sampled)\n",
    "        if n_flow_list[line_idx//n_size_bucket_list_output][line_idx%n_size_bucket_list_output]>=N_FLOW_THRESHOLD[line_idx%n_size_bucket_list_output]:\n",
    "            for _ in range(n_freq):\n",
    "                df_mlsys[line_idx%n_size_bucket_list_output].extend(data)\n",
    "        # else:\n",
    "        #     print(\"skip: \",data[0],data[-1])\n",
    "df_mlsys_shape=[len(df_mlsys[i]) for i in range(len(df_mlsys))]\n",
    "\n",
    "sizes=df_ns3['size']\n",
    "bin=np.digitize(sizes, bin_size_list)\n",
    "\n",
    "bin_counts = np.bincount(bin)\n",
    "total_count = np.sum(bin_counts)\n",
    "# # Calculate the ratio for each bucket\n",
    "bucket_ratios = bin_counts / total_count\n",
    "print(\"bucket_ratios: \",bucket_ratios)\n",
    "bucket_ratios_sampled=n_flow_list_sum/sum(n_flow_list_sum)\n",
    "print(\"bucket_ratios_sampled: \",bucket_ratios_sampled)\n",
    "            \n",
    "df_mlsys_bucket=np.array([np.percentile(df_mlsys[i],99) for i in range(len(df_mlsys))])\n",
    "print(\"df_mlsys_bucket: \",df_mlsys_bucket)\n",
    "# df_mlsys_p99=np.sum(np.multiply(df_mlsys_bucket.T, bucket_ratios_sampled).T,axis=0)\n",
    "df_mlsys_total=[]\n",
    "for i in range(len(df_mlsys)):\n",
    "    # df_mlsys_total.extend(df_mlsys[i])\n",
    "    n_tmp=int(N_FLOWS*bucket_ratios_sampled[i])\n",
    "    df_mlsys_total.extend(np.random.choice(df_mlsys[i],n_tmp,replace=True))\n",
    "df_mlsys_p99=np.percentile(df_mlsys_total,99)\n",
    "\n",
    "sldn_ns3=df_ns3['slowdown']\n",
    "sldn_pmn_m=df_pmn_m['slowdown']\n",
    "sldn_ns3_p99=np.percentile(sldn_ns3,99)\n",
    "sldn_pmn_m_p99=np.percentile(sldn_pmn_m,99)\n",
    "\n",
    "print(\"sldn_ns3: \",sldn_ns3_p99,\" sldn_pmn_m: \", sldn_pmn_m_p99,\" df_mlsys: \", df_mlsys_p99)\n",
    "error=(sldn_pmn_m_p99-sldn_ns3_p99)/sldn_ns3_p99\n",
    "error_mlsys=(df_mlsys_p99-sldn_ns3_p99)/sldn_ns3_p99\n",
    "\n",
    "# assert df_ns3.shape[0]==df_pmn_m.shape[0]==df_ns3_path.shape[0]==sldn_flowsim.shape[0]\n",
    "print(f\"df_ns3: {df_ns3.shape[0]}, df_pmn_m: {df_pmn_m.shape[0]}, df_mlsys: {df_mlsys_shape}\")\n",
    "# assert df_ns3.shape[0]==df_pmn_m.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_fontsize=15\n",
    "plt.figure(0,figsize=(12, 6))\n",
    "plt.suptitle(f\"Slowdown CDF for mix-{worst_low_id} in {mlsys_dir}\\npmn_error={error}, mlsys_error={error_mlsys}\",fontsize=_fontsize+5)\n",
    "plt.rcParams['legend.fontsize'] = _fontsize\n",
    "for i in range(len(labels)):\n",
    "    tmp_sldn_ns3 = np.extract(bin==i, sldn_ns3)\n",
    "    tmp_sldn_pmn_m = np.extract(bin==i, sldn_pmn_m)\n",
    "    tmp_sldn_mlsys=df_mlsys[i]\n",
    "    print(f\"{i}: \", min(tmp_sldn_mlsys))\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.title(f\"{labels[i]}-#{len(tmp_sldn_ns3)}\",fontsize=_fontsize)\n",
    "    plt.plot(np.sort(tmp_sldn_ns3), (np.arange(len(tmp_sldn_ns3))/len(tmp_sldn_ns3)*100), label='ns3', color='blue', linewidth=3)\n",
    "    plt.plot(np.sort(tmp_sldn_pmn_m), (np.arange(len(tmp_sldn_pmn_m))/len(tmp_sldn_pmn_m)*100), label='pmn-m', color='red', linewidth=3)\n",
    "    plt.plot(np.sort(tmp_sldn_mlsys), (np.arange(len(tmp_sldn_mlsys))/len(tmp_sldn_mlsys)*100), label='mlsys', color='green', linewidth=3)\n",
    "    # plt.plot(np.sort(tmp_sldn_flowsim), (np.arange(len(tmp_sldn_flowsim))/len(tmp_sldn_flowsim)*100), label='flowsim', color='orange', linewidth=3)\n",
    "    plt.axhline(99, color='green', linewidth=0.5)\n",
    "    # plt.xscale('log')\n",
    "    plt.ylim(80, 100)\n",
    "    plt.xlabel('slow-down', fontsize=_fontsize)\n",
    "    plt.yticks(fontsize=_fontsize)\n",
    "    plt.xticks(fontsize=_fontsize)\n",
    "    plt.legend()\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
